<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.2/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.17.2/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.2/dist/index.js"></script><script>(r => {
                setTimeout(r);
              })(() => {
  const {
    markmap,
    mm
  } = window;
  const {
    el
  } = markmap.Toolbar.create(mm);
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
            })(() => window.markmap,null,{"content":"ğŸ§  Processamento de Linguagem Natural (PLN)","children":[{"content":"ğŸŒ Analise de Linguagem Natural por Meio de PLN","children":[{"content":"ğŸ“˜ Mas o que Ã© PLN?","children":[{"content":"Processamento de Linguagem Natural (PLN) Ã© uma vertente da inteligÃªncia artificial que ajuda computadores a entender, interpretar e manipular a linguagem humana.","children":[],"payload":{"lines":"6,7"}},{"content":"<strong>AplicaÃ§Ãµes:</strong>","children":[{"content":"Sistema de reconhecimento e comando por voz","children":[],"payload":{"lines":"8,9"}},{"content":"Sistema de recomendaÃ§Ã£o, comando por voz, chatbots","children":[],"payload":{"lines":"9,11"}}],"payload":{"lines":"7,11"}}],"payload":{"lines":"5,6"}},{"content":"ğŸ§© NÃ­veis de Processamento","children":[{"content":"<strong>Morphologia</strong>","children":[],"payload":{"lines":"12,13"}},{"content":"<strong>Sintaxe</strong>","children":[],"payload":{"lines":"13,14"}},{"content":"<strong>SemÃ¢ntica</strong>","children":[],"payload":{"lines":"14,15"}},{"content":"<strong>PragmÃ¡tica</strong>","children":[{"content":"PragmÃ¡tica Ã© o ramo da linguÃ­stica que estuda a linguagem no contexto de seu uso na comunicaÃ§Ã£o.","children":[],"payload":{"lines":"16,17"}},{"content":"Relacionado com a anÃ¡lise detalhada da composiÃ§Ã£o, derivaÃ§Ã£o, flexÃ£o das palavras e seus processos de formaÃ§Ã£o.","children":[],"payload":{"lines":"17,19"}}],"payload":{"lines":"15,19"}}],"payload":{"lines":"11,12"}}],"payload":{"lines":"2,3"}},{"content":"ğŸ§  Deep Learning para PLN","children":[{"content":"ğŸ” Redes de Deep Learning","children":[{"content":"Os primeiros modelos de linguagem usavam arquitetura NN feedforward ou NN convolucional, mas elas nÃ£o capturavam muito bem o contexto.","children":[{"content":"Contexto Ã© como uma palavra ocorre em relaÃ§Ã£o Ã s palavras circundantes na frase.","children":[],"payload":{"lines":"30,31"}},{"content":"Para capturar o contexto, foram aplicados NNs recorrentes.","children":[],"payload":{"lines":"31,32"}},{"content":"O LSTM, uma variante do RNN, foi entÃ£o usada para capturar o contexto de longa distÃ¢ncia. O LSTM bidirecional (BiLSTM) melhora o LSTM ao observar as sequÃªncias de palavras nas direÃ§Ãµes para frente e para trÃ¡s.","children":[],"payload":{"lines":"32,34"}}],"payload":{"lines":"29,34"}}],"payload":{"lines":"28,29"}},{"content":"ğŸŒ Exemplos do Mundo Real em PLN","children":[{"content":"Google substituiu seu sistema de traduÃ§Ã£o baseado em frases pela Neural Machine Translation (NMT), reduzindo os erros de traduÃ§Ã£o em 60%. Ele usa uma rede LSTM profunda com 8 camadas de codificador e 8 de decodificador.","children":[],"payload":{"lines":"35,36"}},{"content":"A revoluÃ§Ã£o na Ã¡rea de NLP com Deep Learning teve inÃ­cio em 2018 com o lanÃ§amento dos modelos de linguagem prÃ©-treinados ELMo e ULMFiT. Mas, foi a proposta de uma nova arquitetura de redes neurais, denominada Transformer, baseada unicamente em mecanismos de atenÃ§Ã£o, que mudaria para sempre as pesquisas nessa Ã¡rea.","children":[{"content":"A arquitetura Transformer permitiu que o treinamento fosse realizado com um volume muito maior de dados do que era possÃ­vel antes. Isso levou ao desenvolvimento de modelos de linguagem prÃ©-treinados, que sÃ£o previamente treinados e, posteriormente, sÃ£o submetidos a um treinamento com ajuste fino (fine-tuning) nas tarefas especÃ­ficas de linguagem.","children":[],"payload":{"lines":"37,39"}}],"payload":{"lines":"36,39"}}],"payload":{"lines":"34,35"}},{"content":"ğŸ“ Mas como isso Ã© possÃ­vel?","children":[{"content":"Os word embeddings sÃ£o representaÃ§Ãµes vetoriais das palavras, que permitem capturar o contexto e relacionamento das palavras nos documentos, sem a necessidade de realizar engenharia de features com anotaÃ§Ãµes exaustivas nas sentenÃ§as.","children":[],"payload":{"lines":"40,42"}}],"payload":{"lines":"39,40"}},{"content":"ğŸŒŸ Marco HistÃ³rico","children":[{"content":"O momento ImageNet, em 2012, marcou o inÃ­cio de um enorme interesse de pesquisadores e empresas no mundo todo por Deep Learning.","children":[],"payload":{"lines":"43,44"}},{"content":"O ano de 2018 determinou o inÃ­cio da revoluÃ§Ã£o na Ã¡rea de NLP com os modelos de linguagem prÃ©-treinados, como ELMo, GPT e BERT, que produziram avanÃ§os significativos em vÃ¡rias tarefas de linguagem natural, tais como inferÃªncia, anÃ¡lise de sentimento e traduÃ§Ã£o de linguagem, em um curto espaÃ§o de tempo.","children":[],"payload":{"lines":"44,45"}}],"payload":{"lines":"42,43"}}],"payload":{"lines":"19,20"}}],"payload":{"lines":"0,1"}},{})</script>
</body>
</html>
