# ğŸ§  Processamento de Linguagem Natural (PLN)

## ğŸŒ Analise de Linguagem Natural por Meio de PLN
- Acontece por Deep Learning

### ğŸ“˜ Mas o que Ã© PLN?
- Processamento de Linguagem Natural (PLN) Ã© uma vertente da inteligÃªncia artificial que ajuda computadores a entender, interpretar e manipular a linguagem humana.
- **AplicaÃ§Ãµes:**
  - Sistema de reconhecimento e comando por voz
  - Sistema de recomendaÃ§Ã£o, comando por voz, chatbots

### ğŸ§© NÃ­veis de Processamento
- **Morphologia**
- **Sintaxe**
- **SemÃ¢ntica**
- **PragmÃ¡tica**
  - PragmÃ¡tica Ã© o ramo da linguÃ­stica que estuda a linguagem no contexto de seu uso na comunicaÃ§Ã£o.
  - Relacionado com a anÃ¡lise detalhada da composiÃ§Ã£o, derivaÃ§Ã£o, flexÃ£o das palavras e seus processos de formaÃ§Ã£o.

## ğŸ§  Deep Learning para PLN
- Sistema de interpretaÃ§Ã£o de linguagem natural
  - Treinamento de cada palavra e contexto.
  - O sistema de NLP permite que a tecnologia usada nÃ£o apenas entenda o significado literal de cada palavra que estÃ¡ sendo dita, como tambÃ©m considere aspectos como:
    - Contexto da conversa
    - Significado sintÃ¡tico e semÃ¢ntico
    - InterpretaÃ§Ã£o dos textos
    - AnÃ¡lise de sentimento e mais

### ğŸ” Redes de Deep Learning
- Os primeiros modelos de linguagem usavam arquitetura NN feedforward ou NN convolucional, mas elas nÃ£o capturavam muito bem o contexto.
  - Contexto Ã© como uma palavra ocorre em relaÃ§Ã£o Ã s palavras circundantes na frase.
  - Para capturar o contexto, foram aplicados NNs recorrentes.
  - O LSTM, uma variante do RNN, foi entÃ£o usada para capturar o contexto de longa distÃ¢ncia. O LSTM bidirecional (BiLSTM) melhora o LSTM ao observar as sequÃªncias de palavras nas direÃ§Ãµes para frente e para trÃ¡s.

### ğŸŒ Exemplos do Mundo Real em PLN
- Google substituiu seu sistema de traduÃ§Ã£o baseado em frases pela Neural Machine Translation (NMT), reduzindo os erros de traduÃ§Ã£o em 60%. Ele usa uma rede LSTM profunda com 8 camadas de codificador e 8 de decodificador.
- A revoluÃ§Ã£o na Ã¡rea de NLP com Deep Learning teve inÃ­cio em 2018 com o lanÃ§amento dos modelos de linguagem prÃ©-treinados ELMo e ULMFiT. Mas, foi a proposta de uma nova arquitetura de redes neurais, denominada Transformer, baseada unicamente em mecanismos de atenÃ§Ã£o, que mudaria para sempre as pesquisas nessa Ã¡rea.
  - A arquitetura Transformer permitiu que o treinamento fosse realizado com um volume muito maior de dados do que era possÃ­vel antes. Isso levou ao desenvolvimento de modelos de linguagem prÃ©-treinados, que sÃ£o previamente treinados e, posteriormente, sÃ£o submetidos a um treinamento com ajuste fino (fine-tuning) nas tarefas especÃ­ficas de linguagem.

### ğŸ“ Mas como isso Ã© possÃ­vel?
- Os word embeddings sÃ£o representaÃ§Ãµes vetoriais das palavras, que permitem capturar o contexto e relacionamento das palavras nos documentos, sem a necessidade de realizar engenharia de features com anotaÃ§Ãµes exaustivas nas sentenÃ§as.

### ğŸŒŸ Marco HistÃ³rico
- O momento ImageNet, em 2012, marcou o inÃ­cio de um enorme interesse de pesquisadores e empresas no mundo todo por Deep Learning.
- O ano de 2018 determinou o inÃ­cio da revoluÃ§Ã£o na Ã¡rea de NLP com os modelos de linguagem prÃ©-treinados, como ELMo, GPT e BERT, que produziram avanÃ§os significativos em vÃ¡rias tarefas de linguagem natural, tais como inferÃªncia, anÃ¡lise de sentimento e traduÃ§Ã£o de linguagem, em um curto espaÃ§o de tempo.
